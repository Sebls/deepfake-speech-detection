# Multilingual Deepfake Speech Detection Evaluation Summary

## **Summary**

This report evaluates the HM-Conformer deepfake speech detection model across **eight languages** in a zero-shot setting, using the model exactly as released and without any fine-tuning. The goal was to assess how well a detector trained primarily on **English ASVspoof 2019 LA data** generalizes to multilingual real and synthetic speech.

**The results show substantial language-dependent performance variability**, with Equal Error Rate (EER) values ranging from **21.40% to 46.19%**, revealing that cross-lingual generalization is highly uneven.

**Ukrainian achieved the best performance**, delivering:

* **Lowest EER (21.40%)**
* **Highest accuracy (84.56%)**
* **Highest ROC AUC (0.83)**
  indicating strong transferability despite no language-specific training.

**Russian and English also performed well**, with EERs below 30% and strong F1 scores, suggesting that the model generalizes more effectively to languages that share acoustic or phonetic characteristics with English, the training language.

In contrast, **Spanish and Polish demonstrated near-random performance**, with EERs above 44% and ROC AUC values close to 0.56, indicating substantial degradation in discriminative capability.

Across most languages, the model shows a **bias toward predicting synthetic audio**, likely inherited from the spoof-heavy distribution in the original ASVspoof training dataset. This results in:

* **High recall for fake samples**
* **Low recall for real samples**
* **High precision for real speech**, but only when the model rarely predicts it

These patterns appear prominently in imbalanced test sets but are rooted in the modelâ€™s training behavior rather than evaluation mechanics.

Overall, the evaluation reveals that HM-Conformer captures **some language-agnostic deepfake cues**, but its performance varies significantly across linguistic groups. 


## Original Training Dataset Information

### HM-Conformer Architecture

The HM-Conformer (Hierarchical Pooling and Multi-level Classification Token Aggregation Conformer) is a Conformer-based audio deepfake detection system that was originally designed for the ASVspoof challenge. The model architecture incorporates:

1. **Hierarchical Pooling Method**: Progressively reduces sequence length to eliminate duplicated information, making it more suitable for classification tasks (many-to-one) rather than sequence-to-sequence tasks.

2. **Multi-level Classification Token Aggregation (MCA)**: Utilizes classification tokens from different Conformer blocks to gather information from various sequence lengths and time compression levels.

### Training Dataset: ASVspoof 2019 Logical Access Task

The model was trained on the **ASVspoof 2019 Logical Access (LA) task** dataset:

- **Training Partition:**
  - 5,128 bona-fide (real) utterances
  - 45,096 spoof utterances
  - Generated by **six different spoofing attack algorithms**
  - Total: 50,224 training samples

- **Development Partition:**
  - Used for validation during training
  - Part of the ASVspoof 2019 LA dataset

### Original Model Performance

The HM-Conformer achieved **15.71% EER** on the ASVspoof 2021 Deepfake (DF) evaluation dataset, which consists of 611,829 samples from 100 spoofed and bona-fide utterance combinations. This performance was competitive with recent systems and represented approximately a 16% improvement over the baseline Conformer (18.91% EER).

### Important Context for Multilingual Evaluation

**Key observations:**
- The model was trained exclusively on **ASVspoof 2019 LA data**, which is primarily English-language content
- The training dataset had a significant class imbalance (8.8:1 ratio of spoof to bona-fide samples)
- The model was not explicitly trained on multilingual data (we're gonna see that yet it demonstrates varying degrees of generalization across 8 different languages)
- The original training focused on detecting spoofing attacks from six specific attack algorithms, which may not fully represent the diversity of actual deepfake generation methods across different languages

This context will help to explain why some languages (particularly those linguistically closer to English or with similar phonetic characteristics) may perform better than others in our multilingual evaluation.

## Methodology

This report presents the evaluation results of the HM-Conformer model across 8 different languages. All evaluations were conducted in test mode using the original model parameters from the original HM-Conformer repository, without any fine-tuning or parameter modifications. The model was tested on language-filtered subsets of the test dataset to assess its performance on each language independently.

## Overall Results Summary

| Language | Test Samples | EER (%) | Accuracy (%) | F1 Score | ROC AUC | Best Metric |
|----------|-------------|---------|---------------|----------|---------|-------------|
| **Ukrainian** | 16,817 | **21.40** | **84.56** | 0.7775 | **0.8257** | Best overall |
| **Russian** | 13,183 | **27.53** | 75.78 | 0.7482 | 0.7631 | 2nd best EER |
| **English** | 89,475 | 29.33 | 83.75 | **0.8943** | 0.7646 | Best F1 score |
| **French** | 46,895 | 30.50 | 72.00 | 0.7155 | 0.7437 | Moderate |
| **German** | 53,468 | 35.55 | 63.12 | 0.6008 | 0.6957 | Moderate |
| **Italian** | 37,624 | 37.05 | 64.18 | 0.6680 | 0.6859 | Moderate |
| **Polish** | 16,040 | 44.55 | 57.84 | 0.6553 | 0.5790 | Lower performance |
| **Spanish** | 30,498 | **46.19** | **49.71** | 0.5738 | **0.5609** | Worst overall |

### Detailed Metrics by Language

#### Ukrainian (Best Performance)
- **EER:** 21.40% (lowest)
- **Accuracy:** 84.56% (highest)
- **F1 Score:** 0.7775
- **ROC AUC:** 0.8257 (highest)
- **Precision:** 0.80, **Recall:** 0.76
- **Test Set:** 10,817 Real, 6,000 Fake

#### Russian (Second Best)
- **EER:** 27.53%
- **Accuracy:** 75.78%
- **F1 Score:** 0.7482
- **ROC AUC:** 0.7631
- **Precision:** 0.84, **Recall:** 0.68
- **Test Set:** 6,183 Real, 7,000 Fake

#### English
- **EER:** 29.33%
- **Accuracy:** 83.75%
- **F1 Score:** 0.8943 (highest F1)
- **ROC AUC:** 0.7646
- **Precision:** 0.88, **Recall:** 0.90
- **Test Set:** 21,475 Real, 68,000 Fake (largest dataset)

#### French
- **EER:** 30.50%
- **Accuracy:** 72.00%
- **F1 Score:** 0.7155
- **ROC AUC:** 0.7437
- **Precision:** 0.87 (Real), 0.61 (Fake)
- **Test Set:** 27,895 Real, 19,000 Fake

#### German
- **EER:** 35.55%
- **Accuracy:** 63.12%
- **F1 Score:** 0.6008
- **ROC AUC:** 0.6957
- **Precision:** 0.90 (Real), 0.46 (Fake)
- **Test Set:** 36,468 Real, 17,000 Fake

#### Italian
- **EER:** 37.05%
- **Accuracy:** 64.18%
- **F1 Score:** 0.6680
- **ROC AUC:** 0.6859
- **Precision:** 0.88 (Real), 0.53 (Fake)
- **Test Set:** 22,624 Real, 15,000 Fake

#### Polish
- **EER:** 44.55%
- **Accuracy:** 57.84%
- **F1 Score:** 0.6553
- **ROC AUC:** 0.5790 (near random)
- **Precision:** 0.64 (Real), 0.55 (Fake)
- **Test Set:** 8,040 Real, 8,000 Fake (most balanced)

#### Spanish (Worst Performance)
- **EER:** 46.19% (highest/worst)
- **Accuracy:** 49.71% (lowest, near random)
- **F1 Score:** 0.5738
- **ROC AUC:** 0.5609 (lowest, near random)
- **Precision:** 0.74 (Real), 0.43 (Fake)
- **Test Set:** 18,498 Real, 12,000 Fake


## Key Findings

### Best Performing Language: Ukrainian

Ukrainian demonstrates **exceptional performance** with the best metrics across the board:
- **Lowest EER (21.40%)** - indicating superior ability to distinguish real from fake audio
- **Highest accuracy (84.56%)** - best overall classification performance
- **Highest ROC AUC (0.83)** - excellent discriminative ability
- **Balanced performance** across both classes with consistent precision and recall

The model shows strong generalization to Ukrainian despite it not being the primary training language, suggesting the HM-Conformer architecture captures language-agnostic features relevant to deepfake detection.

### Worst Performing Language: Spanish

Spanish shows **significant challenges** with performance near random chance:
- **Highest EER (46.19%)** - poor discrimination between real and fake
- **Lowest accuracy (49.71%)** - barely better than random guessing
- **Lowest ROC AUC (0.56)** - minimal discriminative ability
- **High recall for fake (0.86) but very low precision (0.43)** - tendency to over-classify as fake
- **Very low recall for real (0.26)** - misses most real samples

This suggests the model may need language-specific fine-tuning or additional training data for Spanish.

## General Analysis

### Performance Distribution

The results show a **clear performance gradient** across languages:

1. **Top Tier (EER < 30%)**: Ukrainian, Russian, English
   - Strong performance with EER below 30%
   - ROC AUC above 0.76
   - These languages show the model generalizes well

2. **Middle Tier (EER 30-38%)**: French, German, Italian
   - Moderate performance with EER between 30-38%
   - ROC AUC between 0.69-0.74
   - Acceptable but with room for improvement

3. **Lower Tier (EER > 40%)**: Polish, Spanish
   - Challenging performance with EER above 40%
   - ROC AUC near or below 0.58
   - May require language-specific optimization

### Common Patterns

1. **Class Imbalance Effects**: Languages with significant class imbalance (e.g., English with 68K fake vs 21K real) show different precision/recall patterns compared to balanced datasets (e.g., Polish with 8K each).

2. **Fake Detection Bias**: Most languages show higher recall for fake samples, suggesting the model is more aggressive in detecting synthetic audio, sometimes at the cost of misclassifying real samples.

3. **Real Sample Precision**: Many languages (French, German, Italian) show high precision for real samples (0.87-0.90), meaning when the model predicts "real", it's usually correct, but it misses many real samples (low recall).

4. **Dataset Size Impact**: The largest dataset (English, 89K samples) shows strong performance, but smaller datasets like Russian (13K) and Ukrainian (17K) also perform excellently, suggesting dataset size alone doesn't determine performance.


